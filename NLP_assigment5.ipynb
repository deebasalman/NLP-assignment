{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What are Sequence-to-sequence models?"
      ],
      "metadata": {
        "id": "IbSXfsQxD_F9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence to Sequence models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc"
      ],
      "metadata": {
        "id": "msHkgNbXEJ5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the Problem with Vanilla RNNs?"
      ],
      "metadata": {
        "id": "OuOfF0zyENWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is due to the problem of vanishing or exploding gradients, which occurs when the backpropagation algorithm fails to update the weights of the network effectively. Vanilla RNNs have limited representational power, which can make it difficult for them to learn complex patterns in sequential data."
      ],
      "metadata": {
        "id": "fpdXi1r7EQbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Gradient clipping?"
      ],
      "metadata": {
        "id": "46shaHhSEcjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient clipping involves forcing the gradient values (element-wise) to a specific minimum or maximum value if the gradient exceeded an expected range. Together, these methods are often simply referred to as “gradient clipping.”"
      ],
      "metadata": {
        "id": "2Sso7uQrEgOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain Attention mechanism"
      ],
      "metadata": {
        "id": "UZxgy48PEuDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention mechanisms allow the model to selectively focus on the parts of the input that are most important for making a prediction, and to ignore the less relevant parts. This can help the model to make more accurate predictions and to run more efficiently."
      ],
      "metadata": {
        "id": "pDHPHCLAE2l6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain Conditional random fields (CRFs)"
      ],
      "metadata": {
        "id": "WoEMk1zVE3Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account."
      ],
      "metadata": {
        "id": "-J1-R2gHE9u-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain self-attention"
      ],
      "metadata": {
        "id": "cLTL0s5HFJFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self Attention, also called intra Attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation."
      ],
      "metadata": {
        "id": "pvI2Iz4JFL1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Bahdanau Attention?"
      ],
      "metadata": {
        "id": "9qKn04xiFTlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahdanau attention mechanism\n",
        "proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states."
      ],
      "metadata": {
        "id": "kIjmvQppFWtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is a Language Model?"
      ],
      "metadata": {
        "id": "1-W6lb0xFfN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language modeling (LM) is the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence. Language models analyze bodies of text data to provide a basis for their word predictions."
      ],
      "metadata": {
        "id": "9kLriT3xFkrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is Multi-Head Attention?"
      ],
      "metadata": {
        "id": "3ed0XddfFp-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension."
      ],
      "metadata": {
        "id": "pyDqQsiXFswf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is Bilingual Evaluation Understudy (BLEU)"
      ],
      "metadata": {
        "id": "7n05Inq3F2zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score is a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations."
      ],
      "metadata": {
        "id": "X6Or_VnhF6CI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIa4PwhoD8MG"
      },
      "outputs": [],
      "source": []
    }
  ]
}