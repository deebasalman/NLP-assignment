{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What are Vanilla autoencoders"
      ],
      "metadata": {
        "id": "WvGxE6lDGtvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An autoencoder is an unsupervised learning technique for neural networks that learns efficient data representations (encoding) by training the network to ignore signal “noise.” Autoencoders can be used for image denoising, image compression, and, in some cases, even generation of image data."
      ],
      "metadata": {
        "id": "-e3N94vGHDNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are Sparse autoencoders"
      ],
      "metadata": {
        "id": "0eKZKGQmHM4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Sparse Autoencoder is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer."
      ],
      "metadata": {
        "id": "nGXUxb_QHQ2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are Denoising autoencoders"
      ],
      "metadata": {
        "id": "e7vPvxhYHYqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Denoising Autoencoder is a modification on the autoencoder to prevent the network learning the identity function. Specifically, if the autoencoder is too big, then it can just learn the data, so the output equals the input, and does not perform any useful representation learning or dimensionality reduction."
      ],
      "metadata": {
        "id": "leGlJpDsHcJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Convolutional autoencoders"
      ],
      "metadata": {
        "id": "cBnRSZDCHkFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional autoencoders (CAEs) are unsupervised dimensionality reduction models composed by convolutional layers capable of creating compressed image representations"
      ],
      "metadata": {
        "id": "_h1Pjf2jHobL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are Stacked autoencoders"
      ],
      "metadata": {
        "id": "3fLI06m3H-Iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked autoencoder is a multi-layer neural network that consists of multiple autoencoders, where the output of each encoder gets fed into the next encoder until the last encoder feeds its output into a chain of decoders."
      ],
      "metadata": {
        "id": "hzqoVFulIBmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain how to generate sentences using LSTM autoencoders"
      ],
      "metadata": {
        "id": "H68r4rw-IWYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " For encoder-decoder, input is squashed into a single feature vector, if we want our output to regenerate the same dimension as the original input, we need to convert this single feature vector 1D to 2D by replicating it using RepeatVector() ."
      ],
      "metadata": {
        "id": "_OJiJgnrIbB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Explain Extractive summarization"
      ],
      "metadata": {
        "id": "P45qz8w7JE15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extractive summarization techniques vary, yet they all share the same basic tasks:\n",
        "\n",
        "1-Construct an intermediate representation of the input text (text to be summarized)\n",
        "\n",
        "2-Score the sentences based on the constructed intermediate representation.\n",
        "\n",
        "3-Select a summary consisting of the top k most important sentences."
      ],
      "metadata": {
        "id": "ZMmHjwU-JIC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Explain Abstractive summarization"
      ],
      "metadata": {
        "id": "cEzkA6arJksf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a semantic representation of the document in our brains. We then pick words from our general vocabulary (the words we commonly use) that fit in the semantics, to create a short summary that represents all the points of the actual document."
      ],
      "metadata": {
        "id": "mfqFDGdpJlme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain Beam search"
      ],
      "metadata": {
        "id": "oU3Ud_eDKGzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic. But in beam search, only a predetermined number of best partial solutions are kept as candidates."
      ],
      "metadata": {
        "id": "ix55jqcaKOCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Explain Length normalization"
      ],
      "metadata": {
        "id": "473BSb9yKO3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document length normalization adjusts the term frequency or the relevance score in order to normalize the effect of document length on the document ranking"
      ],
      "metadata": {
        "id": "UR6nhicbKR66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain Coverage normalization"
      ],
      "metadata": {
        "id": "oPyCoZvBKY2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It uses a threshold-based algorithm where it orders the reads either randomly (default) or in order of alignment accuracy, and incrementally keeps reads which have at least one base that has not yet reached a user-defined coverage threshold.\n",
        "\n",
        "This gives the algorithm a few properties:\n",
        "\n",
        "*For any position in the genome with total coverage less than the threshold, all reads spanning it will be kept\n",
        "\n",
        "*For any position in the genome with total coverage greater than or equal to the threshold, its coverage among the kept reads will not drop below the threshold."
      ],
      "metadata": {
        "id": "FuniX1IxKcRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Explain ROUGE metric evaluation"
      ],
      "metadata": {
        "id": "BYMN4tliLjO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing."
      ],
      "metadata": {
        "id": "y9JKUV7nLkPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XGey2etoL73c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKRpW3ONGsIe"
      },
      "outputs": [],
      "source": []
    }
  ]
}