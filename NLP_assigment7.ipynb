{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the architecture of BERT"
      ],
      "metadata": {
        "id": "pX-JY1LZMd14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT Model Architecture: BERT is released in two sizes BERTBASE and BERTLARGE. The BASE model is used to measure the performance of the architecture comparable to another architecture and the LARGE model produces state-of-the-art results that were reported in the research paper."
      ],
      "metadata": {
        "id": "m9FgmU8GMf-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain Masked Language Modeling (MLM)"
      ],
      "metadata": {
        "id": "lbFgfm9bMvl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLM is a language model trained to predict the missing words in a sentence based on the context provided by the surrounding words. This is done by masking some of the words in the input text and training the model to predict the masked words based on the context of the non-masked words."
      ],
      "metadata": {
        "id": "XmxIbtoKMymf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain Next Sentence Prediction (NSP)"
      ],
      "metadata": {
        "id": "FcqTSuiRM7V6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to predict if the two sentences were following each other or not."
      ],
      "metadata": {
        "id": "f1SiT69GM-69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Matthews evaluation?"
      ],
      "metadata": {
        "id": "KGaGJ4sCNOAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table."
      ],
      "metadata": {
        "id": "rKJviobDNjTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Matthews Correlation Coefficient (MCC)?"
      ],
      "metadata": {
        "id": "xpujWiYMNlwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Matthews Correlation Coefficient (MCC) is one of the popular measurements for classification accuracy. It has been generally regarded as a balanced measure which can be used even if the classes are of very different sizes."
      ],
      "metadata": {
        "id": "tiVS5PE0N2fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain Semantic Role Labeling"
      ],
      "metadata": {
        "id": "QPAapYQSN7K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Role Labeling (SRL):SRL is also called shallow semantic parsing. In SRL, labels are assigned to words or phrases in a sentence that indicate their semantic role in the sentence."
      ],
      "metadata": {
        "id": "Uh3pfvsCN-Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Why Fine-tuning a BERT model takes less time than pretraining"
      ],
      "metadata": {
        "id": "-OMSDBmYOKdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks."
      ],
      "metadata": {
        "id": "mw5fA8HwOkwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Recognizing Textual Entailment (RTE)"
      ],
      "metadata": {
        "id": "EgnI5_bLOnXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textual entailment recognition is the task of deciding, given two text fragments, whether the meaning of one text is entailed (can be inferred) from another text. This task captures generically a broad range of inferences that are relevant for multiple applications."
      ],
      "metadata": {
        "id": "GmLkd_25QVk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the decoder stack of GPT models."
      ],
      "metadata": {
        "id": "qgPKQMvTQZIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The number of bits in output code is more than the bits in its input code. The most commonly used practical binary decoders are 2-to-4 decoder, 3-to-8 decoder and 4-to-16 line binary decoder."
      ],
      "metadata": {
        "id": "NK3Ev8ebQjeX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZoF2ge_MdP-"
      },
      "outputs": [],
      "source": []
    }
  ]
}