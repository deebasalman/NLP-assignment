{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain One-Hot Encoding"
      ],
      "metadata": {
        "id": "caVesialruI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Hot Encoding can be defined as a process of transforming categorical variables into numerical format before fitting and training a Machine Learning algorithm. For each categorical variable, One Hot Encoding produces a numeric vector with a length equal to the number of categories present in the feature."
      ],
      "metadata": {
        "id": "Kytcnx5IrwgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain Bag of Words"
      ],
      "metadata": {
        "id": "YvugtogBr0o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity."
      ],
      "metadata": {
        "id": "_89UJwEIsKh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain Bag of N-Grams"
      ],
      "metadata": {
        "id": "NxbUTdUUsLhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bag-of-n-grams model records the number of times that each n-gram appears in each document of a collection. An n-gram is a collection of n successive words. bagOfNgrams does not split text into words."
      ],
      "metadata": {
        "id": "mdnbaS9usO3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain TF-IDF"
      ],
      "metadata": {
        "id": "1P7gneAhtADc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents"
      ],
      "metadata": {
        "id": "F5J2Xhj9s1A7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is OOV problem?"
      ],
      "metadata": {
        "id": "nXbrYBVRtDfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out-of-Vocabulary (OOV) is a problem for Neural Machine Translation (NMT). OOV refers to words with a low occurrence in the training data, or to those that are absent from the training data. To alleviate this, word or phrase-based Data Augmentation (DA) techniques have been used."
      ],
      "metadata": {
        "id": "KlFOtCJDtGaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are word embeddings?"
      ],
      "metadata": {
        "id": "Bqyr6fc-tQGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In natural language processing (NLP), a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning."
      ],
      "metadata": {
        "id": "wroqvkTjteYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Explain Continuous bag of words (CBOW)"
      ],
      "metadata": {
        "id": "fE7Hj2a7thoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW (Continuous Bag of Words) model predicts a target word based on the context of the surrounding words in a sentence or text. It is trained using a feedforward neural network where the input is a set of context words, and the output is the target word."
      ],
      "metadata": {
        "id": "CHi7EEUJtik3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Explain SkipGram"
      ],
      "metadata": {
        "id": "2iGXrC39tpp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SkipGram is an algorithm that is used to create word embeddings i.e. high-dimensional vector representation of words. These embeddings are meant to encode the semantic meaning of words such that words that are semantically similar will lie close to each other in that vector's space."
      ],
      "metadata": {
        "id": "tV_oDEAyt31J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain Glove Embeddings."
      ],
      "metadata": {
        "id": "L7HjumDxt4vJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic idea behind the GloVe word embedding is to derive the relationship between the words from statistics. Unlike the occurrence matrix, the co-occurrence matrix tells you how often a particular word pair occurs together. Each value in the co-occurrence matrix represents a pair of words occurring together."
      ],
      "metadata": {
        "id": "1nCmH4fat95i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umuugCkyrkNt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4lCWoDEjrthz"
      }
    }
  ]
}