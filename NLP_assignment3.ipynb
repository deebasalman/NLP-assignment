{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the basic architecture of RNN cell."
      ],
      "metadata": {
        "id": "71kIlM6G5vaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An RNN comprises a series of repeating neural network \"cells\" that are connected in a chain-like structure, where the output of one cell is passed as input to the next cell. Each cell inputs the current input to the network and the hidden state from the previous time step, producing an output and a new hidden state."
      ],
      "metadata": {
        "id": "8f0EyjDl5xSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain Backpropagation through time (BPTT)"
      ],
      "metadata": {
        "id": "2t9iSkS-5-E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers."
      ],
      "metadata": {
        "id": "r-YXrZKv6Fw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain Vanishing and exploding gradients"
      ],
      "metadata": {
        "id": "WNS4Tvav6Hak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two opposite scenarios could happen in this case: the derivative term gets extremely small, i.e., approaches zero vs. this term gets extremely large and overflows. These issues are referred to as the Vanishing and Exploding"
      ],
      "metadata": {
        "id": "mxyHL8CR6LCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain Long short-term memory (LSTM)"
      ],
      "metadata": {
        "id": "ldJGQGWp6fhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "' LSTM stands for long short-term memory networks, used in the field of Deep Learning. It is a variety of recurrent neural networks (RNNs) that are capable of learning long-term dependencies, especially in sequence prediction problems."
      ],
      "metadata": {
        "id": "UCf8BZmL6i8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain Gated recurrent unit (GRU)"
      ],
      "metadata": {
        "id": "z3WF-dNc6qJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gated Recurrent Unit (GRU) networks process sequential data, such as time series or natural language, bypassing the hidden state from one time step to the next. The hidden state is a vector that captures the information from the past time steps relevant to the current time step."
      ],
      "metadata": {
        "id": "OBJltCQN6t1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain Peephole LSTM"
      ],
      "metadata": {
        "id": "xhRRdtNy63uM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. In this peephole connection we can see that all the gates are having an input along with the cell state"
      ],
      "metadata": {
        "id": "7XXxhu1Q68yD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Bidirectional RNNs"
      ],
      "metadata": {
        "id": "oEMYRHNl7dxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bidirectional recurrent neural network (RNN) is a type of recurrent neural network (RNN) that processes input sequences in both forward and backward directions. This allows the RNN to capture information from the input sequence that may be relevant to the output prediction."
      ],
      "metadata": {
        "id": "HC1IcDUD7szC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Explain the gates of LSTM with equations."
      ],
      "metadata": {
        "id": "qmM5gJKA71Nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " LSTM have 3 gates:\n",
        "\n",
        "1) Input Gate.\n",
        "\n",
        "2) Forget Gate.\n",
        "\n",
        "3) Output Gate.\n",
        "\n",
        "Gates in LSTM are the sigmoid activation functions i.e they output a value between 0 or 1 and in most of the cases it is either 0 or 1.\n",
        "\n",
        "\n",
        "\n",
        "h(t-1) and c(t-1) are the inputs from the previous timestep LSTM. o(t) is the output of the LSTM for this timestep. The LSTM also generates the c(t) and h(t) for the consumption of the next time step LSTM."
      ],
      "metadata": {
        "id": "nIdHC3B_81C5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain BiLSTM"
      ],
      "metadata": {
        "id": "k-tf_T6h-Ka-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction."
      ],
      "metadata": {
        "id": "cJ_m-gTTCftu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Explain BiGRU"
      ],
      "metadata": {
        "id": "GU0T24bt9F3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates."
      ],
      "metadata": {
        "id": "bfPCZ2WkDVLc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpNrd6jP6Gxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To0QxvOL5sda"
      },
      "outputs": [],
      "source": []
    }
  ]
}